{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#some requirements\n",
    "!pip install loralib\n",
    "!pip install sentencepiece\n",
    "!pip install sacrebleu\n",
    "!pip install peft\n",
    "'''\n",
    "myrank = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AdamW, get_scheduler, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sacrebleu.metrics import BLEU\n",
    "import time\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "#data prepare and process\n",
    "f = open(\"chinese.txt\", \"r\")\n",
    "cndata = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(\"english.txt\", \"r\")\n",
    "endata = f.readlines()\n",
    "f.close()\n",
    "\n",
    "assert len(cndata) == len(endata) == 252777\n",
    "\n",
    "mydata = [{\"cn\": cn.strip(), \"en\": en.strip()} for cn, en in zip(cndata, endata)]\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, mydata) -> None:\n",
    "        super().__init__()\n",
    "        self.data = mydata\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "mydata = Mydataset(mydata)\n",
    "train_size = int(0.8 * 252777)\n",
    "val_size = 252777 - train_size\n",
    "trainset, valset = random_split(mydata, lengths=[train_size, val_size])\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "trainable params: 589824 || all params: 78533120 || trainable%: 0.7510512762004107\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, force_download=True, resume_download=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=myrank, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "model = model.to(device)\n",
    "\n",
    "assert model.model.model.encoder.layers[0].self_attn.v_proj.lora_A.default.weight.requires_grad == True\n",
    "assert model.model.model.encoder.layers[0].self_attn.k_proj.weight.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycollate_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['cn'])\n",
    "        batch_targets.append(sample['en'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=mycollate_fn)\n",
    "valid_dataloader = DataLoader(valset, batch_size=32, shuffle=False, collate_fn=mycollate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f30f0715a049119c7eaf2e8f45eb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6321, training time 1330.0534136295319\n"
     ]
    }
   ],
   "source": [
    "#only optimize lora layers\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad_, model.parameters()), lr=3e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "progress_bar.set_description(f'loss: {0:>7f}')\n",
    "\n",
    "total_loss = 0.0\n",
    "losses = []\n",
    "\n",
    "i = 1\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    t1 = time.time()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/i:>7f}')\n",
    "        progress_bar.update(1)\n",
    "        i += 1\n",
    "    t2 = time.time()\n",
    "    print(\"epoch {}, training time {}\".format(i, t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"translate_peft_lora.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('translate_peft_lora.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a89723de4b8410497ad9bb318e0f06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Until the late 1960’s, a harmonious balance between international openness and national ownership guaranteed universal prosperity.\n",
      "['Until the late 1960s, a harmonious balance of international openness and national autonomy allowed for widespread prosperity.']\n",
      "In 2009, that figure could exceed $50 billion.\n",
      "['The figure for 2009 could exceed $50 billion.']\n",
      "The term “decisive” is used to characterize market-based roads as being of similar significance.\n",
      "['The Third Plenum’s use of “decisive” to anchor a market-based approach is a similar gambit.']\n",
      "Bent Flyvbjerg, a professor at Oxford University dedicated to project management and planning, studied 70 years of data, and concluded with a “massive engineering iron law”: they would almost inevitably “overspent, overspent, overspent, overspent, overspent, and overspent.”\n",
      "['Bent Flyvbjerg, a professor at the University of Oxford specialized in program management and planning, studied 70 years of data to conclude that there is an “iron law of megaprojects”: they are almost invariably “over budget, over time, over and over again.”']\n",
      "BLEU: 37.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, labels = [], []\n",
    "from sacrebleu import BLEU\n",
    "import numpy as np\n",
    "bleu = BLEU()\n",
    "\n",
    "model.eval()\n",
    "i = 50\n",
    "for batch_data in tqdm(valid_dataloader):\n",
    "    i -= 1\n",
    "    batch_data = batch_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "        ).cpu().numpy()\n",
    "    label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "    preds += [pred.strip() for pred in decoded_preds]\n",
    "    labels += [[label.strip()] for label in decoded_labels]\n",
    "    if i <= 0:\n",
    "        break\n",
    "\n",
    "for i in range(4):\n",
    "    print(preds[i])\n",
    "    print(labels[i])\n",
    "bleu_score = bleu.corpus_score(preds, labels).score\n",
    "print(f\"BLEU: {bleu_score:>0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lora_pred.txt\", 'w') as f:\n",
    "    for i in preds:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "with open(\"lora_label.txt\", 'w') as f:\n",
    "    for i in labels:\n",
    "        f.write(i[0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5349,     2,   907,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [  132, 28609, 41412,     2, 16351,  9128,  8677,  5257, 47772, 16351,\n",
      "             0, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000],\n",
      "        [    7, 22070, 30331,    69,  7001,  2049, 22070,   291, 19838,  7355,\n",
      "         18682,     2,    74, 22070,   291, 19838, 19838,  8092,     2,  1956,\n",
      "         37868,  3606, 30331,  2086,     9,     0],\n",
      "        [    7,  7925,  1878,   577, 40382, 17460,   660, 12736, 12154,     2,\n",
      "           716,   150, 12307,  1003, 10054,   863,    80,  3681,  5989, 20660,\n",
      "          2977,     9,     0, 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0]])}\n",
      "['Hello, world.', 'You love me. I love you, honey.', 'Kyoto is a city located south of Japan’s capital, governed by it, and one of the cities designated by decree.', 'Translation is an example that highlights the advantages of attention mechanisms, the context of which is crucial to the meaning of a word in a sentence.']\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(model, tokenizer, sentence, device, max_target_length=128):\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "    print(inputs)\n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Decode the generated tokens to a string\n",
    "    decoded_translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_translation\n",
    "\n",
    "# Example usage\n",
    "model.eval() # Make sure the model is in evaluation mode\n",
    "sentence = [\"你好，世界\", \"你爱我我爱你，蜜雪冰城甜蜜蜜\", \"京都市是位于日本京都府南部的城市，为京都府府治，也是政令指定都市之一。\", \"翻译是一个能够突显注意力机制优势的例子，其中上下文对于句子中单词的含义至关重要。\"] # Your Chinese sentence\n",
    "translation = translate_sentence(model, tokenizer, sentence, device)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mlosses\u001b[49m))), losses)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
