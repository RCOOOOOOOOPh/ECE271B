{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#some requirements\n",
    "!pip install loralib\n",
    "!pip install sentencepiece\n",
    "!pip install sacrebleu\n",
    "!pip install peft\n",
    "'''\n",
    "myrank = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AdamW, get_scheduler, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sacrebleu.metrics import BLEU\n",
    "import time\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "#data prepare and process\n",
    "f = open(\"chinese.txt\", \"r\")\n",
    "cndata = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open(\"english.txt\", \"r\")\n",
    "endata = f.readlines()\n",
    "f.close()\n",
    "\n",
    "assert len(cndata) == len(endata) == 252777\n",
    "\n",
    "mydata = [{\"cn\": cn.strip(), \"en\": en.strip()} for cn, en in zip(cndata, endata)]\n",
    "\n",
    "class Mydataset(Dataset):\n",
    "    def __init__(self, mydata) -> None:\n",
    "        super().__init__()\n",
    "        self.data = mydata\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "mydata = Mydataset(mydata)\n",
    "train_size = int(0.8 * 252777)\n",
    "val_size = 252777 - train_size\n",
    "trainset, valset = random_split(mydata, lengths=[train_size, val_size])\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_input_length = 128\n",
    "max_target_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "trainable params: 608256 || all params: 78551552 || trainable%: 0.7743398882812653\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, force_download=True, resume_download=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "config = LoraConfig(\n",
    "    use_dora=True,\n",
    "    r=myrank, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "model = model.to(device)\n",
    "\n",
    "assert model.model.model.encoder.layers[0].self_attn.v_proj.lora_A.default.weight.requires_grad == True\n",
    "assert model.model.model.encoder.layers[0].self_attn.k_proj.weight.requires_grad == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mycollate_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample['cn'])\n",
    "        batch_targets.append(sample['en'])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs,\n",
    "        padding=True,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_targets,\n",
    "            padding=True,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n",
    "        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "        for idx, end_idx in enumerate(end_token_index):\n",
    "            labels[idx][end_idx+1:] = -100\n",
    "        batch_data['labels'] = labels\n",
    "    return batch_data\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=mycollate_fn)\n",
    "valid_dataloader = DataLoader(valset, batch_size=32, shuffle=False, collate_fn=mycollate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997ce680175b4ecb8ad7e9d35f5c724c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6321, training time 1672.9888784885406\n"
     ]
    }
   ],
   "source": [
    "#only optimize lora layers\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad_, model.parameters()), lr=3e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "progress_bar.set_description(f'loss: {0:>7f}')\n",
    "\n",
    "total_loss = 0.0\n",
    "losses = []\n",
    "\n",
    "i = 1\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    t1 = time.time()\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/i:>7f}')\n",
    "        progress_bar.update(1)\n",
    "        i += 1\n",
    "    t2 = time.time()\n",
    "    print(\"epoch {}, training time {}\".format(i, t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"translate_peft_dora.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('translate_peft_dora.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819a82320a644fa484f0bc6b4359a741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given India’s popular desire for regional peace and religious reconciliation, it is no surprise that the BJP, which represents Hinduism, has failed.\n",
      "['As for the threat from the right, anyone hoping for peace in the region and reduced tensions within India between religious communities is relieved by the defeat of the Hindu nationalist Bharatiya Janata Party (BJP).']\n",
      "Economic growth and increased energy demand would allow air pollution emissions to increase steadily, and rapidly increase the concentration of particulate matter (PM) and ozone over the next few decades, so that such an approach would not be desirable.\n",
      "['With economic growth and rising energy demand set to fuel a steady rise in emissions of air pollutants and rapidly rising concentrations of particulate matter (PM) and ozone in the coming decades, this approach is untenable.']\n",
      "The US could and should implement a policy that would allow real income to grow faster, but that would have to be discussed in the next column.\n",
      "['The US can and should adopt policies that will cause real incomes to rise even faster. But that is a subject for a future column.']\n",
      "For its part, the US is actively shifting “rebalancing” to Asia.\n",
      "['On its part, the United States is now “rebalancing” toward Asia.']\n",
      "BLEU: 13.38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds, labels = [], []\n",
    "from sacrebleu import BLEU\n",
    "import numpy as np\n",
    "bleu = BLEU()\n",
    "\n",
    "model.eval()\n",
    "i = 50\n",
    "for batch_data in tqdm(valid_dataloader):\n",
    "    i -= 1\n",
    "    batch_data = batch_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "        ).cpu().numpy()\n",
    "    label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "    preds += [pred.strip() for pred in decoded_preds]\n",
    "    labels += [[label.strip()] for label in decoded_labels]\n",
    "    if i <= 0:\n",
    "        break\n",
    "\n",
    "for i in range(4):\n",
    "    print(preds[i])\n",
    "    print(labels[i])\n",
    "bleu_score = bleu.corpus_score(preds, labels).score\n",
    "print(f\"BLEU: {bleu_score:>0.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dora_pred.txt\", 'w') as f:\n",
    "    for i in preds:\n",
    "        f.write(i+'\\n')\n",
    "\n",
    "with open(\"dora_label.txt\", 'w') as f:\n",
    "    for i in labels:\n",
    "        f.write(i[0]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5349,     2,   907,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000],\n",
      "        [  132, 28609, 41412,     2, 16351,  9128,  8677,  5257, 47772, 16351,\n",
      "             0, 65000, 65000, 65000],\n",
      "        [    7, 19558,  1592,   166,  1546,    11,     2,  2421,  1281,  1281,\n",
      "          1018, 44624,     9,     0],\n",
      "        [    7, 26628,    65, 12609, 37834,  1027,  1058,   146,  7727,    11,\n",
      "         19679,     9,     0, 65000]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "Hello, world.\n",
      "You love me. I love you, honey.\n",
      "Man is born free and bound everywhere.\n",
      "Integrity and diligence should be your permanent companions.\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(model, tokenizer, sentence, device, max_target_length=128):\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "    print(inputs)\n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "        ).cpu().numpy()\n",
    "\n",
    "    # Decode the generated tokens to a string\n",
    "    decoded_translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_translation\n",
    "\n",
    "# Example usage\n",
    "model.eval() # Make sure the model is in evaluation mode\n",
    "sentence = [\"你好，世界\", \n",
    "            \"你爱我我爱你，蜜雪冰城甜蜜蜜\", \n",
    "            \"人是生而自由的，却处处受到束缚。\", \n",
    "            \"诚实与勤勉应该成为你永久的伴侣。\"] # Your Chinese sentence\n",
    "translation = translate_sentence(model, tokenizer, sentence, device)\n",
    "for tt in translation:\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mlosses\u001b[49m))), losses)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(list(range(len(losses))), losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
