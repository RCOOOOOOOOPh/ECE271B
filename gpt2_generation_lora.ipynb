{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AdamW, get_scheduler, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import re\n",
    "import time\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "myrank=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for e2e_nlg_cleaned contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/e2e_nlg_cleaned\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304bf34670ad43009b4ee6dc5049f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5f6c1735494dcf9275a31f6d39ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17744da331c4defbaa4e1d25ee731ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cebeab44f14576ab5327309c21468b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/112k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634f4dc3e36f4e49afda2c79fe2ed960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/133k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4d207fa86e45eaaa0ccdcbfd58fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/33525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28577a4f0a874816af1b9870589980e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa028ddc4e14b86adb43484433bae21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f59c3868eb437c951c2340016dfdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0cdf52ab2a4dc39d880777e9e00c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8688f5285dba41a09352527aa12b4d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"left\"\n",
    "\n",
    "raw_dataset = load_dataset(\"e2e_nlg_cleaned\")\n",
    "def mytokenize(example):\n",
    "    ans = []\n",
    "    for i, rep in enumerate(example[\"meaning_representation\"]):\n",
    "        ans.append(\"Write a restaurant description for the following attributes:\\n\" + rep + \"\\n\" + \"Description: \" + example[\"human_reference\"][i])\n",
    "    return tokenizer(ans, truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "dataset = raw_dataset.map(mytokenize, batched=True, remove_columns=['meaning_representation', 'human_reference'])\n",
    "'''print(dataset)\n",
    "print(dataset[\"train\"][0])'''\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.tensor([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.tensor([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.tensor([item['input_ids'] for item in batch])  # Labels are input_ids for language modeling\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(dataset['validation'], batch_size=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_checkpoint)\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589824 || all params: 125029632 || trainable%: 0.4717473694555863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:861: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdaa6bfd19d4bb4be8dd933130d7fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7129, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5852, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5952, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3773, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.6648, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 9.3136, 'learning_rate': 1.2e-05, 'epoch': 0.0}\n",
      "{'loss': 9.645, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 9.6284, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}\n",
      "{'loss': 9.2037, 'learning_rate': 1.8e-05, 'epoch': 0.0}\n",
      "{'loss': 9.4466, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 9.2932, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 9.5133, 'learning_rate': 2.4e-05, 'epoch': 0.01}\n",
      "{'loss': 9.357, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}\n",
      "{'loss': 9.5052, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 9.4774, 'learning_rate': 3e-05, 'epoch': 0.01}\n",
      "{'loss': 9.2953, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 9.1951, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}\n",
      "{'loss': 9.4107, 'learning_rate': 3.6e-05, 'epoch': 0.01}\n",
      "{'loss': 9.4018, 'learning_rate': 3.8e-05, 'epoch': 0.01}\n",
      "{'loss': 9.1177, 'learning_rate': 4e-05, 'epoch': 0.01}\n",
      "{'loss': 9.0005, 'learning_rate': 4.2e-05, 'epoch': 0.01}\n",
      "{'loss': 8.9172, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 9.0213, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 8.8012, 'learning_rate': 4.8e-05, 'epoch': 0.01}\n",
      "{'loss': 8.705, 'learning_rate': 5e-05, 'epoch': 0.01}\n",
      "{'loss': 8.8476, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 8.7299, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 8.9526, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 8.8734, 'learning_rate': 5.8e-05, 'epoch': 0.01}\n",
      "{'loss': 8.563, 'learning_rate': 6e-05, 'epoch': 0.01}\n",
      "{'loss': 8.4714, 'learning_rate': 6.2e-05, 'epoch': 0.01}\n",
      "{'loss': 8.4294, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 8.2052, 'learning_rate': 6.6e-05, 'epoch': 0.02}\n",
      "{'loss': 8.0895, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 7.9868, 'learning_rate': 7e-05, 'epoch': 0.02}\n",
      "{'loss': 7.7878, 'learning_rate': 7.2e-05, 'epoch': 0.02}\n",
      "{'loss': 7.7916, 'learning_rate': 7.4e-05, 'epoch': 0.02}\n",
      "{'loss': 7.6095, 'learning_rate': 7.6e-05, 'epoch': 0.02}\n",
      "{'loss': 7.3352, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 7.2656, 'learning_rate': 8e-05, 'epoch': 0.02}\n",
      "{'loss': 6.9923, 'learning_rate': 8.2e-05, 'epoch': 0.02}\n",
      "{'loss': 6.7418, 'learning_rate': 8.4e-05, 'epoch': 0.02}\n",
      "{'loss': 6.5928, 'learning_rate': 8.6e-05, 'epoch': 0.02}\n",
      "{'loss': 6.2727, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 6.1007, 'learning_rate': 9e-05, 'epoch': 0.02}\n",
      "{'loss': 5.8382, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 5.786, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 5.7524, 'learning_rate': 9.4e-05, 'epoch': 0.02}\n",
      "{'loss': 5.5763, 'learning_rate': 9.6e-05, 'epoch': 0.02}\n",
      "{'loss': 5.0612, 'learning_rate': 9.8e-05, 'epoch': 0.02}\n",
      "{'loss': 4.7556, 'learning_rate': 0.0001, 'epoch': 0.02}\n",
      "{'loss': 4.2738, 'learning_rate': 0.0001, 'epoch': 0.02}\n",
      "{'loss': 4.4711, 'learning_rate': 0.00010200000000000001, 'epoch': 0.03}\n",
      "{'loss': 4.0503, 'learning_rate': 0.00010400000000000001, 'epoch': 0.03}\n",
      "{'loss': 3.8535, 'learning_rate': 0.00010600000000000002, 'epoch': 0.03}\n",
      "{'loss': 3.4643, 'learning_rate': 0.00010800000000000001, 'epoch': 0.03}\n",
      "{'loss': 3.1176, 'learning_rate': 0.00011000000000000002, 'epoch': 0.03}\n",
      "{'loss': 2.95, 'learning_rate': 0.00011200000000000001, 'epoch': 0.03}\n",
      "{'loss': 2.425, 'learning_rate': 0.00011399999999999999, 'epoch': 0.03}\n",
      "{'loss': 2.1228, 'learning_rate': 0.000116, 'epoch': 0.03}\n",
      "{'loss': 1.6862, 'learning_rate': 0.000118, 'epoch': 0.03}\n",
      "{'loss': 1.459, 'learning_rate': 0.00012, 'epoch': 0.03}\n",
      "{'loss': 1.2725, 'learning_rate': 0.000122, 'epoch': 0.03}\n",
      "{'loss': 0.9233, 'learning_rate': 0.000124, 'epoch': 0.03}\n",
      "{'loss': 0.7952, 'learning_rate': 0.000126, 'epoch': 0.03}\n",
      "{'loss': 0.6276, 'learning_rate': 0.00012800000000000002, 'epoch': 0.03}\n",
      "{'loss': 0.5019, 'learning_rate': 0.00013000000000000002, 'epoch': 0.03}\n",
      "{'loss': 0.457, 'learning_rate': 0.000132, 'epoch': 0.03}\n",
      "{'loss': 0.4541, 'learning_rate': 0.000134, 'epoch': 0.03}\n",
      "{'loss': 0.4665, 'learning_rate': 0.00013600000000000003, 'epoch': 0.03}\n",
      "{'loss': 0.4188, 'learning_rate': 0.000138, 'epoch': 0.03}\n",
      "{'loss': 0.457, 'learning_rate': 0.00014, 'epoch': 0.03}\n",
      "{'loss': 0.4463, 'learning_rate': 0.000142, 'epoch': 0.03}\n",
      "{'loss': 0.4756, 'learning_rate': 0.000144, 'epoch': 0.04}\n",
      "{'loss': 0.4574, 'learning_rate': 0.000146, 'epoch': 0.04}\n",
      "{'loss': 0.4879, 'learning_rate': 0.000148, 'epoch': 0.04}\n",
      "{'loss': 0.4211, 'learning_rate': 0.00015000000000000001, 'epoch': 0.04}\n",
      "{'loss': 0.394, 'learning_rate': 0.000152, 'epoch': 0.04}\n",
      "{'loss': 0.385, 'learning_rate': 0.000154, 'epoch': 0.04}\n",
      "{'loss': 0.406, 'learning_rate': 0.00015600000000000002, 'epoch': 0.04}\n",
      "{'loss': 0.3982, 'learning_rate': 0.00015800000000000002, 'epoch': 0.04}\n",
      "{'loss': 0.349, 'learning_rate': 0.00016, 'epoch': 0.04}\n",
      "{'loss': 0.3435, 'learning_rate': 0.000162, 'epoch': 0.04}\n",
      "{'loss': 0.3718, 'learning_rate': 0.000164, 'epoch': 0.04}\n",
      "{'loss': 0.3761, 'learning_rate': 0.000166, 'epoch': 0.04}\n",
      "{'loss': 0.3543, 'learning_rate': 0.000168, 'epoch': 0.04}\n",
      "{'loss': 0.3541, 'learning_rate': 0.00017, 'epoch': 0.04}\n",
      "{'loss': 0.3437, 'learning_rate': 0.000172, 'epoch': 0.04}\n",
      "{'loss': 0.3127, 'learning_rate': 0.000174, 'epoch': 0.04}\n",
      "{'loss': 0.3449, 'learning_rate': 0.00017600000000000002, 'epoch': 0.04}\n",
      "{'loss': 0.3114, 'learning_rate': 0.00017800000000000002, 'epoch': 0.04}\n",
      "{'loss': 0.3266, 'learning_rate': 0.00018, 'epoch': 0.04}\n",
      "{'loss': 0.3114, 'learning_rate': 0.000182, 'epoch': 0.04}\n",
      "{'loss': 0.3168, 'learning_rate': 0.00018400000000000003, 'epoch': 0.04}\n",
      "{'loss': 0.2974, 'learning_rate': 0.00018600000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.3084, 'learning_rate': 0.000188, 'epoch': 0.05}\n",
      "{'loss': 0.3134, 'learning_rate': 0.00019, 'epoch': 0.05}\n",
      "{'loss': 0.3198, 'learning_rate': 0.000192, 'epoch': 0.05}\n",
      "{'loss': 0.329, 'learning_rate': 0.000194, 'epoch': 0.05}\n",
      "{'loss': 0.2977, 'learning_rate': 0.000196, 'epoch': 0.05}\n",
      "{'loss': 0.2941, 'learning_rate': 0.00019800000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.3036, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 0.2797, 'learning_rate': 0.00019800000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.2897, 'learning_rate': 0.000196, 'epoch': 0.05}\n",
      "{'loss': 0.3121, 'learning_rate': 0.000194, 'epoch': 0.05}\n",
      "{'loss': 0.3195, 'learning_rate': 0.000192, 'epoch': 0.05}\n",
      "{'loss': 0.2856, 'learning_rate': 0.00019, 'epoch': 0.05}\n",
      "{'loss': 0.3077, 'learning_rate': 0.000188, 'epoch': 0.05}\n",
      "{'loss': 0.2857, 'learning_rate': 0.00018600000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.3029, 'learning_rate': 0.00018400000000000003, 'epoch': 0.05}\n",
      "{'loss': 0.2881, 'learning_rate': 0.000182, 'epoch': 0.05}\n",
      "{'loss': 0.2899, 'learning_rate': 0.00018, 'epoch': 0.05}\n",
      "{'loss': 0.286, 'learning_rate': 0.00017800000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.2892, 'learning_rate': 0.00017600000000000002, 'epoch': 0.05}\n",
      "{'loss': 0.2882, 'learning_rate': 0.000174, 'epoch': 0.05}\n",
      "{'loss': 0.2757, 'learning_rate': 0.000172, 'epoch': 0.06}\n",
      "{'loss': 0.3009, 'learning_rate': 0.00017, 'epoch': 0.06}\n",
      "{'loss': 0.2983, 'learning_rate': 0.000168, 'epoch': 0.06}\n",
      "{'loss': 0.2999, 'learning_rate': 0.000166, 'epoch': 0.06}\n",
      "{'loss': 0.2796, 'learning_rate': 0.000164, 'epoch': 0.06}\n",
      "{'loss': 0.2939, 'learning_rate': 0.000162, 'epoch': 0.06}\n",
      "{'loss': 0.2981, 'learning_rate': 0.00016, 'epoch': 0.06}\n",
      "{'loss': 0.2871, 'learning_rate': 0.00015800000000000002, 'epoch': 0.06}\n",
      "{'loss': 0.2584, 'learning_rate': 0.00015600000000000002, 'epoch': 0.06}\n",
      "{'loss': 0.2751, 'learning_rate': 0.000154, 'epoch': 0.06}\n",
      "{'loss': 0.2714, 'learning_rate': 0.000152, 'epoch': 0.06}\n",
      "{'loss': 0.2572, 'learning_rate': 0.00015000000000000001, 'epoch': 0.06}\n",
      "{'loss': 0.2486, 'learning_rate': 0.000148, 'epoch': 0.06}\n",
      "{'loss': 0.2847, 'learning_rate': 0.000146, 'epoch': 0.06}\n",
      "{'loss': 0.2695, 'learning_rate': 0.000144, 'epoch': 0.06}\n",
      "{'loss': 0.2503, 'learning_rate': 0.000142, 'epoch': 0.06}\n",
      "{'loss': 0.2765, 'learning_rate': 0.00014, 'epoch': 0.06}\n",
      "{'loss': 0.2783, 'learning_rate': 0.000138, 'epoch': 0.06}\n",
      "{'loss': 0.2649, 'learning_rate': 0.00013600000000000003, 'epoch': 0.06}\n",
      "{'loss': 0.2504, 'learning_rate': 0.000134, 'epoch': 0.06}\n",
      "{'loss': 0.2653, 'learning_rate': 0.000132, 'epoch': 0.06}\n",
      "{'loss': 0.2805, 'learning_rate': 0.00013000000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.2803, 'learning_rate': 0.00012800000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.2774, 'learning_rate': 0.000126, 'epoch': 0.07}\n",
      "{'loss': 0.2623, 'learning_rate': 0.000124, 'epoch': 0.07}\n",
      "{'loss': 0.2548, 'learning_rate': 0.000122, 'epoch': 0.07}\n",
      "{'loss': 0.2613, 'learning_rate': 0.00012, 'epoch': 0.07}\n",
      "{'loss': 0.2624, 'learning_rate': 0.000118, 'epoch': 0.07}\n",
      "{'loss': 0.2705, 'learning_rate': 0.000116, 'epoch': 0.07}\n",
      "{'loss': 0.2333, 'learning_rate': 0.00011399999999999999, 'epoch': 0.07}\n",
      "{'loss': 0.2529, 'learning_rate': 0.00011200000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.2527, 'learning_rate': 0.00011000000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.2703, 'learning_rate': 0.00010800000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.2398, 'learning_rate': 0.00010600000000000002, 'epoch': 0.07}\n",
      "{'loss': 0.2638, 'learning_rate': 0.00010400000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.2887, 'learning_rate': 0.00010200000000000001, 'epoch': 0.07}\n",
      "{'loss': 0.25, 'learning_rate': 0.0001, 'epoch': 0.07}\n",
      "{'loss': 0.2512, 'learning_rate': 9.8e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2518, 'learning_rate': 9.6e-05, 'epoch': 0.07}\n",
      "{'loss': 0.252, 'learning_rate': 9.4e-05, 'epoch': 0.07}\n",
      "{'loss': 0.245, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2752, 'learning_rate': 9e-05, 'epoch': 0.07}\n",
      "{'loss': 0.246, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2769, 'learning_rate': 8.6e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2537, 'learning_rate': 8.4e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2672, 'learning_rate': 8.2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2386, 'learning_rate': 8e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2597, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2263, 'learning_rate': 7.6e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2628, 'learning_rate': 7.4e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2699, 'learning_rate': 7.2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2431, 'learning_rate': 7e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2571, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2506, 'learning_rate': 6.6e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2588, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.26, 'learning_rate': 6.2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2413, 'learning_rate': 6e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2423, 'learning_rate': 5.8e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2723, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2304, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2357, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2359, 'learning_rate': 5e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2589, 'learning_rate': 4.8e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2552, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.234, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2445, 'learning_rate': 4.2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2549, 'learning_rate': 4e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2524, 'learning_rate': 3.8e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2518, 'learning_rate': 3.6e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2345, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2464, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2694, 'learning_rate': 3e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2353, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2605, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.09}\n",
      "{'loss': 0.251, 'learning_rate': 2.4e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2507, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2311, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2187, 'learning_rate': 1.8e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2272, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2369, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2569, 'learning_rate': 1.2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2247, 'learning_rate': 1e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2413, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 0.2507, 'learning_rate': 6e-06, 'epoch': 0.09}\n",
      "{'loss': 0.2465, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'train_runtime': 556.0954, 'train_samples_per_second': 5.754, 'train_steps_per_second': 0.36, 'train_loss': 2.491693864390254, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=2.491693864390254, metrics={'train_runtime': 556.0954, 'train_samples_per_second': 5.754, 'train_steps_per_second': 0.36, 'train_loss': 2.491693864390254, 'epoch': 0.1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=200,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no]\n",
      "Description: Alimentum is not a family-friendly place in the city centre.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no]\n",
      "Description: Alimentum in city centre is not a family-friendly place.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]\n",
      "Description: Alimentum is not family-friendly, and is near the Burger King in the city centre.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]\n",
      "Description: Near Burger King in city centre is the adult establishment Alimentum.\n"
     ]
    }
   ],
   "source": [
    "mybatch = None\n",
    "i = 2\n",
    "for b in valid_loader:\n",
    "    mybatch = b\n",
    "    i -= 1\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "test_text = tokenizer.batch_decode(mybatch['input_ids'], skip_special_tokens=True)\n",
    "for d in test_text:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybatch[\"input_ids\"].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\generation\\utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\generation\\utils.py:1363: UserWarning: Input length of input_ids is 1024, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\torch\\utils\\checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no]\n",
      "Description: Alimentum is not a family-friendly place in the city centre.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no]\n",
      "Description: Alimentum in city centre is not a family-friendly place.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]\n",
      "Description: Alimentum is not family-friendly, and is near the Burger King in the city centre.\n",
      "Write a restaurant description for the following attributes:\n",
      "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]\n",
      "Description: Near Burger King in city centre is the adult establishment Alimentum.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "for k in mybatch.keys():\n",
    "    mybatch[k] = mybatch[k].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(**mybatch).cpu().numpy()\n",
    "\n",
    "decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "for dec in decoded_preds:\n",
    "    print(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a restaurant description for the following attributes:\n",
      " name[Alimentum],  familyFriendly[no]Write a restaurant description for the following attributes:\n",
      " name[Alimentum], area[Shimokitazawa], near[Senpai's house]Write\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\myml\\Lib\\site-packages\\transformers\\generation\\utils.py:1363: UserWarning: Input length of input_ids is 55, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_text = [\n",
    "    \"Write a restaurant description for the following attributes:\\n name[Alimentum], area[city centre], familyFriendly[no]\"\n",
    "    \"Write a restaurant description for the following attributes:\\n name[Alimentum], area[Shimokitazawa], near[Senpai's house]\"\n",
    "]\n",
    "tokenized_test = tokenizer(test_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(**tokenized_test).cpu().numpy()\n",
    "\n",
    "decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "for dec in decoded_preds:\n",
    "    print(dec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
