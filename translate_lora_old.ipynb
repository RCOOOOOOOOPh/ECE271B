{"cells":[{"cell_type":"code","source":["!pip install loralib\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-JRYG_6mObo","executionInfo":{"status":"ok","timestamp":1709262043643,"user_tz":480,"elapsed":21153,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"b7c0f0d8-9689-4a3b-a88f-c53319169b92"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2a8fK_i-L_6y","executionInfo":{"status":"ok","timestamp":1709262075241,"user_tz":480,"elapsed":6522,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}}},"outputs":[],"source":["from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, AutoModelWithLMHead, get_scheduler\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, random_split, DataLoader\n","from tqdm.auto import tqdm\n","import loralib as lora\n","import mylora\n","import re\n","import time"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKh68ZRUL_69","executionInfo":{"status":"ok","timestamp":1709262077691,"user_tz":480,"elapsed":1088,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"ac52d26a-0644-4cd8-fdeb-2cde77174c0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["252777\n","252777\n","{'cn': '除了评估之前错误的原因之外，报告指出，任何一个新兴经济体的决策者都应该在推行产业政策之前回答三个基本问题：', 'en': 'Beyond assessing what went wrong previously, the report identifies three fundamental questions that policymakers in any emerging economy should answer before pursuing industrial policy:'}\n"]}],"source":["f = open(\"chinese.txt\", \"r\")\n","cndata = f.readlines()\n","f.close()\n","\n","f = open(\"english.txt\", \"r\")\n","endata = f.readlines()\n","f.close()\n","\n","print(len(cndata))\n","print(len(endata))\n","\n","mydata = [{\"cn\": cn.strip(), \"en\": en.strip()} for cn, en in zip(cndata, endata)]\n","print(mydata[114514])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gLWta_GL_6_","executionInfo":{"status":"ok","timestamp":1709262078963,"user_tz":480,"elapsed":186,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"50e3bbbf-77d7-458b-ddb4-88db727e0ac3"},"outputs":[{"output_type":"stream","name":"stdout","text":["202221 50556\n","{'cn': 'lamp#160;amp#160;amp#160;amp#160;amp#160;amp#160;amp#160; 中国有大约1000家太阳热能公司，营业收入25亿美元，下属60万工人从事制造和安装工作。', 'en': '·China has some 1,000 solar thermal energy firms, generating sales of $2.5 billion and employing 600,000 workers in manufacturing and installation.'}\n"]}],"source":["class Mydataset(Dataset):\n","    def __init__(self, mydata) -> None:\n","        super().__init__()\n","        self.data = mydata\n","    def __len__(self):\n","        return len(self.data)\n","    def __getitem__(self, index):\n","        return self.data[index]\n","\n","mydata = Mydataset(mydata)\n","train_size = int(0.8 * 252777)\n","val_size = 252777 - train_size\n","trainset, valset = random_split(mydata, lengths=[train_size, val_size])\n","print(len(trainset), len(valset))\n","print(trainset[114514])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIvcC39lL_7C","executionInfo":{"status":"ok","timestamp":1709262082238,"user_tz":480,"elapsed":973,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"61eb2abf-798f-4e4b-87d8-0daac77ac0aa"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]}],"source":["model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YNqrkB9iL_7F","executionInfo":{"status":"ok","timestamp":1709262083407,"user_tz":480,"elapsed":190,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"6be7ec1f-17a7-4d67-c2e7-8fbb931b2b6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["lamp#160;amp#160;amp#160;amp#160;amp#160;amp#160;amp#160; 中国有大约1000家太阳热能公司，营业收入25亿美元，下属60万工人从事制造和安装工作。\n","·China has some 1,000 solar thermal energy firms, generating sales of $2.5 billion and employing 600,000 workers in manufacturing and installation.\n","{'input_ids': [3993, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 15466, 3631, 11636, 25, 7214, 123, 3351, 29559, 1208, 13812, 4567, 533, 1026, 2, 22722, 1497, 666, 2051, 2, 18430, 1839, 1810, 3510, 1917, 4726, 16, 9587, 193, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","{'input_ids': [7, 222, 17574, 64, 239, 11616, 20845, 45627, 1504, 10861, 2, 17145, 10813, 4, 51998, 3152, 6, 28230, 52853, 1748, 10, 11838, 6, 18074, 5, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","['▁l', 'amp', '#', '160', ';', 'amp', '#', '160', ';', 'amp', '#', '160', ';', 'amp', '#', '160', ';', 'amp', '#', '160', ';', 'amp', '#', '160', ';', 'amp', '#', '160', ';', '▁中国', '有', '大约', '1000', '家', '太阳', '热', '能', '公司', ',', '营业', '收入', '25', '亿美元', ',', '下属', '60', '万', '工人', '从事', '制造', '和', '安装', '工作', '。', '</s>']\n","['▁', '·', 'China', '▁has', '▁some', '▁1,000', '▁solar', '▁thermal', '▁energy', '▁firms', ',', '▁generating', '▁sales', '▁of', '▁$2.5', '▁billion', '▁and', '▁employing', '▁600,000', '▁workers', '▁in', '▁manufacturing', '▁and', '▁installation', '.', '</s>']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]}],"source":["cn = trainset[114514][\"cn\"]\n","en = trainset[114514][\"en\"]\n","print(cn)\n","print(en)\n","cn_token = tokenizer(cn)\n","with tokenizer.as_target_tokenizer():\n","    en_token = tokenizer(en)\n","\n","print(cn_token)\n","print(en_token)\n","print(tokenizer.convert_ids_to_tokens(cn_token['input_ids']))\n","print(tokenizer.convert_ids_to_tokens(en_token['input_ids']))"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"td67da_jL_7K","executionInfo":{"status":"ok","timestamp":1709262087737,"user_tz":480,"elapsed":2073,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"0fc9e457-4363-41d1-8bcf-136424e9b297"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"output_type":"stream","name":"stdout","text":["MarianMTModel(\n","  (model): MarianModel(\n","    (shared): Embedding(65001, 512, padding_idx=65000)\n","    (encoder): MarianEncoder(\n","      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): SiLU()\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (decoder): MarianDecoder(\n","      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (activation_fn): SiLU()\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",")\n"]}],"source":["from transformers import AutoModelForSeq2SeqLM\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","#model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, force_download=True, resume_download=False)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n","print(model)"]},{"cell_type":"code","source":["for name, module in model.named_modules():\n","    if ('self_attn' in name or 'self_attention' in name) and isinstance(module, nn.Linear):\n","        print(name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAPAN-CYncq6","executionInfo":{"status":"ok","timestamp":1709261142407,"user_tz":480,"elapsed":157,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"f61f0c67-be77-46f3-b8a9-1f11d9f8e21b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["model.encoder.layers.0.self_attn.k_proj\n","model.encoder.layers.0.self_attn.v_proj\n","model.encoder.layers.0.self_attn.q_proj\n","model.encoder.layers.0.self_attn.out_proj\n","model.encoder.layers.1.self_attn.k_proj\n","model.encoder.layers.1.self_attn.v_proj\n","model.encoder.layers.1.self_attn.q_proj\n","model.encoder.layers.1.self_attn.out_proj\n","model.encoder.layers.2.self_attn.k_proj\n","model.encoder.layers.2.self_attn.v_proj\n","model.encoder.layers.2.self_attn.q_proj\n","model.encoder.layers.2.self_attn.out_proj\n","model.encoder.layers.3.self_attn.k_proj\n","model.encoder.layers.3.self_attn.v_proj\n","model.encoder.layers.3.self_attn.q_proj\n","model.encoder.layers.3.self_attn.out_proj\n","model.encoder.layers.4.self_attn.k_proj\n","model.encoder.layers.4.self_attn.v_proj\n","model.encoder.layers.4.self_attn.q_proj\n","model.encoder.layers.4.self_attn.out_proj\n","model.encoder.layers.5.self_attn.k_proj\n","model.encoder.layers.5.self_attn.v_proj\n","model.encoder.layers.5.self_attn.q_proj\n","model.encoder.layers.5.self_attn.out_proj\n","model.decoder.layers.0.self_attn.k_proj\n","model.decoder.layers.0.self_attn.v_proj\n","model.decoder.layers.0.self_attn.q_proj\n","model.decoder.layers.0.self_attn.out_proj\n","model.decoder.layers.1.self_attn.k_proj\n","model.decoder.layers.1.self_attn.v_proj\n","model.decoder.layers.1.self_attn.q_proj\n","model.decoder.layers.1.self_attn.out_proj\n","model.decoder.layers.2.self_attn.k_proj\n","model.decoder.layers.2.self_attn.v_proj\n","model.decoder.layers.2.self_attn.q_proj\n","model.decoder.layers.2.self_attn.out_proj\n","model.decoder.layers.3.self_attn.k_proj\n","model.decoder.layers.3.self_attn.v_proj\n","model.decoder.layers.3.self_attn.q_proj\n","model.decoder.layers.3.self_attn.out_proj\n","model.decoder.layers.4.self_attn.k_proj\n","model.decoder.layers.4.self_attn.v_proj\n","model.decoder.layers.4.self_attn.q_proj\n","model.decoder.layers.4.self_attn.out_proj\n","model.decoder.layers.5.self_attn.k_proj\n","model.decoder.layers.5.self_attn.v_proj\n","model.decoder.layers.5.self_attn.q_proj\n","model.decoder.layers.5.self_attn.out_proj\n"]}]},{"cell_type":"code","source":["print(model.model.encoder.layers[0].self_attn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85g8GKowxoeN","executionInfo":{"status":"ok","timestamp":1709261144853,"user_tz":480,"elapsed":122,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"c1630b1c-84bd-4a0c-c052-844d0c701d45"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["MarianAttention(\n","  (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","  (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["for name, module in model.named_modules():\n","    if ('self_attn' in name or 'self_attention' in name) and isinstance(module, nn.Linear):\n","        #print(module.in_features, module.out_features)\n","        #take model.encoder.layers.0.self_attn.k_proj as example, the out put is 512 512\n","        parent_name, child_name = name.rsplit('.', 1)\n","        #print(parent_name, child_name)\n","        #model.encoder.layers.0.self_attn k_proj\n","        parent_name = re.sub(r\"\\.(\\d)\", r\"[\\1]\", parent_name)\n","        #model.encoder.layers[0].self_attn\n","        setattr(eval(f'model.{parent_name}'), child_name, mylora.LoRALinear(module.in_features, module.out_features, r=8))\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFwAaC7JnHtc","executionInfo":{"status":"ok","timestamp":1709262096760,"user_tz":480,"elapsed":288,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"982ded59-5638-4a3c-9966-7be14125acc3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["MarianMTModel(\n","  (model): MarianModel(\n","    (shared): Embedding(65001, 512, padding_idx=65000)\n","    (encoder): MarianEncoder(\n","      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianEncoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (v_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (q_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (out_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): SiLU()\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (decoder): MarianDecoder(\n","      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n","      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n","      (layers): ModuleList(\n","        (0-5): 6 x MarianDecoderLayer(\n","          (self_attn): MarianAttention(\n","            (k_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (v_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (q_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","            (out_proj): LoRALinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (activation_fn): SiLU()\n","          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): MarianAttention(\n","            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n","            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",")\n"]}]},{"cell_type":"code","source":["print(model.model.encoder.layers[0].self_attn.k_proj)\n","print(model.model.encoder.layers[0].self_attn.k_proj.__class__)\n","print(model.model.encoder.layers[0].fc1.__class__)\n","#successfully changed to lora layer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cF4B2quE0kqO","executionInfo":{"status":"ok","timestamp":1709261403984,"user_tz":480,"elapsed":128,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"3782c2f9-f037-4898-8cc9-457dd146f81d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["LoRALinear(in_features=512, out_features=512, bias=True)\n","<class 'mylora.LoRALinear'>\n","<class 'torch.nn.modules.linear.Linear'>\n"]}]},{"cell_type":"code","source":["for name, module in model.named_modules():\n","    if isinstance(module, mylora.LoRALinear):\n","        print(name)\n","    else:\n","        module.requires_grad_ = False\n","model = model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzcA7sHj6Orf","executionInfo":{"status":"ok","timestamp":1709262102565,"user_tz":480,"elapsed":363,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"57affd0f-d43c-4b10-bd39-4b823b31d351"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["model.encoder.layers.0.self_attn.k_proj\n","model.encoder.layers.0.self_attn.v_proj\n","model.encoder.layers.0.self_attn.q_proj\n","model.encoder.layers.0.self_attn.out_proj\n","model.encoder.layers.1.self_attn.k_proj\n","model.encoder.layers.1.self_attn.v_proj\n","model.encoder.layers.1.self_attn.q_proj\n","model.encoder.layers.1.self_attn.out_proj\n","model.encoder.layers.2.self_attn.k_proj\n","model.encoder.layers.2.self_attn.v_proj\n","model.encoder.layers.2.self_attn.q_proj\n","model.encoder.layers.2.self_attn.out_proj\n","model.encoder.layers.3.self_attn.k_proj\n","model.encoder.layers.3.self_attn.v_proj\n","model.encoder.layers.3.self_attn.q_proj\n","model.encoder.layers.3.self_attn.out_proj\n","model.encoder.layers.4.self_attn.k_proj\n","model.encoder.layers.4.self_attn.v_proj\n","model.encoder.layers.4.self_attn.q_proj\n","model.encoder.layers.4.self_attn.out_proj\n","model.encoder.layers.5.self_attn.k_proj\n","model.encoder.layers.5.self_attn.v_proj\n","model.encoder.layers.5.self_attn.q_proj\n","model.encoder.layers.5.self_attn.out_proj\n","model.decoder.layers.0.self_attn.k_proj\n","model.decoder.layers.0.self_attn.v_proj\n","model.decoder.layers.0.self_attn.q_proj\n","model.decoder.layers.0.self_attn.out_proj\n","model.decoder.layers.1.self_attn.k_proj\n","model.decoder.layers.1.self_attn.v_proj\n","model.decoder.layers.1.self_attn.q_proj\n","model.decoder.layers.1.self_attn.out_proj\n","model.decoder.layers.2.self_attn.k_proj\n","model.decoder.layers.2.self_attn.v_proj\n","model.decoder.layers.2.self_attn.q_proj\n","model.decoder.layers.2.self_attn.out_proj\n","model.decoder.layers.3.self_attn.k_proj\n","model.decoder.layers.3.self_attn.v_proj\n","model.decoder.layers.3.self_attn.q_proj\n","model.decoder.layers.3.self_attn.out_proj\n","model.decoder.layers.4.self_attn.k_proj\n","model.decoder.layers.4.self_attn.v_proj\n","model.decoder.layers.4.self_attn.q_proj\n","model.decoder.layers.4.self_attn.out_proj\n","model.decoder.layers.5.self_attn.k_proj\n","model.decoder.layers.5.self_attn.v_proj\n","model.decoder.layers.5.self_attn.q_proj\n","model.decoder.layers.5.self_attn.out_proj\n"]}]},{"cell_type":"code","source":["#double check\n","for name, module in model.named_modules():\n","    if module.requires_grad_:\n","        print(name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gsvn42nB593J","executionInfo":{"status":"ok","timestamp":1709262111255,"user_tz":480,"elapsed":227,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"9809e05c-a2e7-41a8-ea63-8f01d8b91c51"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["model.encoder.layers.0.self_attn.k_proj\n","model.encoder.layers.0.self_attn.v_proj\n","model.encoder.layers.0.self_attn.q_proj\n","model.encoder.layers.0.self_attn.out_proj\n","model.encoder.layers.1.self_attn.k_proj\n","model.encoder.layers.1.self_attn.v_proj\n","model.encoder.layers.1.self_attn.q_proj\n","model.encoder.layers.1.self_attn.out_proj\n","model.encoder.layers.2.self_attn.k_proj\n","model.encoder.layers.2.self_attn.v_proj\n","model.encoder.layers.2.self_attn.q_proj\n","model.encoder.layers.2.self_attn.out_proj\n","model.encoder.layers.3.self_attn.k_proj\n","model.encoder.layers.3.self_attn.v_proj\n","model.encoder.layers.3.self_attn.q_proj\n","model.encoder.layers.3.self_attn.out_proj\n","model.encoder.layers.4.self_attn.k_proj\n","model.encoder.layers.4.self_attn.v_proj\n","model.encoder.layers.4.self_attn.q_proj\n","model.encoder.layers.4.self_attn.out_proj\n","model.encoder.layers.5.self_attn.k_proj\n","model.encoder.layers.5.self_attn.v_proj\n","model.encoder.layers.5.self_attn.q_proj\n","model.encoder.layers.5.self_attn.out_proj\n","model.decoder.layers.0.self_attn.k_proj\n","model.decoder.layers.0.self_attn.v_proj\n","model.decoder.layers.0.self_attn.q_proj\n","model.decoder.layers.0.self_attn.out_proj\n","model.decoder.layers.1.self_attn.k_proj\n","model.decoder.layers.1.self_attn.v_proj\n","model.decoder.layers.1.self_attn.q_proj\n","model.decoder.layers.1.self_attn.out_proj\n","model.decoder.layers.2.self_attn.k_proj\n","model.decoder.layers.2.self_attn.v_proj\n","model.decoder.layers.2.self_attn.q_proj\n","model.decoder.layers.2.self_attn.out_proj\n","model.decoder.layers.3.self_attn.k_proj\n","model.decoder.layers.3.self_attn.v_proj\n","model.decoder.layers.3.self_attn.q_proj\n","model.decoder.layers.3.self_attn.out_proj\n","model.decoder.layers.4.self_attn.k_proj\n","model.decoder.layers.4.self_attn.v_proj\n","model.decoder.layers.4.self_attn.q_proj\n","model.decoder.layers.4.self_attn.out_proj\n","model.decoder.layers.5.self_attn.k_proj\n","model.decoder.layers.5.self_attn.v_proj\n","model.decoder.layers.5.self_attn.q_proj\n","model.decoder.layers.5.self_attn.out_proj\n"]}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cFHMXO9SL_7L","executionInfo":{"status":"ok","timestamp":1709262118964,"user_tz":480,"elapsed":159,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}}},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","\n","def mycollate_fn(batch_samples):\n","    batch_inputs, batch_targets = [], []\n","    for sample in batch_samples:\n","        batch_inputs.append(sample['cn'])\n","        batch_targets.append(sample['en'])\n","    batch_data = tokenizer(\n","        batch_inputs,\n","        padding=True,\n","        max_length=max_input_length,\n","        truncation=True,\n","        return_tensors=\"pt\"\n","    )\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            batch_targets,\n","            padding=True,\n","            max_length=max_target_length,\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )[\"input_ids\"]\n","        batch_data['decoder_input_ids'] = model.prepare_decoder_input_ids_from_labels(labels)\n","        end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n","        for idx, end_idx in enumerate(end_token_index):\n","            labels[idx][end_idx+1:] = -100\n","        batch_data['labels'] = labels\n","    return batch_data\n","\n","train_dataloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=mycollate_fn)\n","valid_dataloader = DataLoader(valset, batch_size=32, shuffle=False, collate_fn=mycollate_fn)"]},{"cell_type":"code","source":["for batch in train_dataloader:\n","    break\n","print(batch.keys())\n","for k, v in batch.items():\n","    print(k, v.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9idUg2eNW7b","executionInfo":{"status":"ok","timestamp":1709262129629,"user_tz":480,"elapsed":196,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"bc0ff33b-9128-487e-a705-88e762b02181"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'labels'])\n","input_ids torch.Size([32, 49])\n","attention_mask torch.Size([32, 49])\n","decoder_input_ids torch.Size([32, 61])\n","labels torch.Size([32, 61])\n"]}]},{"cell_type":"code","source":["outputs = model(**batch.to(device))\n","for k, v in outputs.items():\n","    print(k, v.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQpDu4rCOh_Q","executionInfo":{"status":"ok","timestamp":1709262132969,"user_tz":480,"elapsed":1114,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"c87aaac8-ddfc-48cb-f04a-1ac090bfd71f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["loss torch.Size([])\n","logits torch.Size([32, 61, 65001])\n","encoder_last_hidden_state torch.Size([32, 49, 512])\n"]}]},{"cell_type":"code","source":["optimizer = AdamW(filter(lambda p: p.requires_grad_, model.parameters()), lr=3e-5)\n","\n","num_epochs = 2\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","\n","progress_bar = tqdm(range(num_training_steps))\n","progress_bar.set_description(f'loss: {0:>7f}')\n","\n","total_loss = 0.0\n","losses = []\n","\n","i = 1\n","model.train()\n","for epoch in range(num_epochs):\n","    t1 = time.time()\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        losses.append(loss.item())\n","        total_loss += loss.item()\n","        progress_bar.set_description(f'loss: {total_loss/i:>7f}')\n","        progress_bar.update(1)\n","        i += 1\n","    t2 = time.time()\n","    print(\"epoch {}, training time {}\".format(i, t2-t1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506,"referenced_widgets":["0f5f907fb54b4e8fb63f2650ec343b90","1eeb7478f1c84133aa757dde4615cdf1","9833eca0a35e4716b0530c9a29ab5799","1319d4b2b0ba450081cada2760de8176","32747ae4668e4ce8945628ba697214a4","07e0a9b4f88e425794e1791994a51bc4","3e6e53cec35f4c0583765276ea7fc1ac","537d4b9f191f4075933c2126caa726cd","afbf756d5541422d9c1d6aab05f9d204","ee3357ba838c4a48877c57bcab1bb370","6e514657e5c647f8ae091b1e0fbfb289"]},"id":"m5Im3zUJPJtt","executionInfo":{"status":"error","timestamp":1709262769438,"user_tz":480,"elapsed":529429,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"17dfeae5-83d3-4b6f-c3fa-e2c4dc2e7346"},"execution_count":14,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f5f907fb54b4e8fb63f2650ec343b90","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/12640 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-4ddcb9568fca>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"/content/cn-en_translator.pt\")"],"metadata":{"id":"418aFOmBX64L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sacrebleu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OriwvEX9aTSb","executionInfo":{"status":"ok","timestamp":1703397655414,"user_tz":480,"elapsed":12380,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"639d79c2-c67f-41a6-a34d-33e010fa2ea7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/106.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.0\n"]}]},{"cell_type":"code","source":["preds, labels = [], []\n","import numpy as np\n","from sacrebleu.metrics import BLEU\n","bleu = BLEU()\n","\n","model.eval()\n","for batch_data in tqdm(valid_dataloader):\n","    batch_data = batch_data.to(device)\n","    with torch.no_grad():\n","        generated_tokens = model.generate(\n","            batch_data[\"input_ids\"],\n","            attention_mask=batch_data[\"attention_mask\"],\n","            max_length=max_target_length,\n","        ).cpu().numpy()\n","    label_tokens = batch_data[\"labels\"].cpu().numpy()\n","\n","    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","    label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n","\n","    preds += [pred.strip() for pred in decoded_preds]\n","    labels += [[label.strip()] for label in decoded_labels]\n","    print(preds[0])\n","    print(labels[0])\n","    break\n","bleu_score = bleu.corpus_score(preds, labels).score\n","print(f\"BLEU: {bleu_score:>0.2f}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260,"referenced_widgets":["850620df17ac488f88a51d9e2d3f0707","ccc098f5f4974ac2809c774dc8179145","35a2a7714d3a42caa7a082f32b4165bc","7a39e5f32b484b52826de9928b635e79","48943d199a20464c9fcb208832098c0e","af606d616af14b1aaa0cb79c1c0e9013","d759ca2acc07453fa333a8ee9cfc872a","86c9db158c3649649b1b1158e013053a","918201f10e52400fa6de1462ebb149b9","e3c225b570b9439eabf4030009c8cd53","55733f66f1794a5ca9242ec4274cb361"]},"id":"EHO6Qpi6Ziqp","executionInfo":{"status":"ok","timestamp":1703398111827,"user_tz":480,"elapsed":3224,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"f6ba8031-0712-42a2-e287-7f3e21f1153d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1580 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"850620df17ac488f88a51d9e2d3f0707"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n","Why should corporate leaders invest in an uncertain world, rather than pay dividends to meet high-demand (but generally risk-averse) investors, or buy back their own shares (and thus improve their rates of return, if only by raising their own salaries)?\n","['Why would business leaders invest in an uncertain world, rather than paying dividends to demanding (but generally risk-averse) investors, or buying back some of their companies’ own shares (thereby improving the price/earnings ratio and, better yet, increasing their own remuneration)?']\n","BLEU: 40.01\n","\n"]}]},{"cell_type":"code","source":["def translate_sentence(model, tokenizer, sentence, device, max_target_length=128):\n","    # Tokenize the input sentence\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n","    print(inputs)\n","    # Move tensors to the same device as the model\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Generate translation using the model\n","    with torch.no_grad():\n","        generated_tokens = model.generate(\n","            inputs[\"input_ids\"],\n","            attention_mask=inputs[\"attention_mask\"],\n","            max_length=max_target_length,\n","        ).cpu().numpy()\n","\n","    # Decode the generated tokens to a string\n","    decoded_translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","\n","    return decoded_translation\n","\n","# Example usage\n","model.eval() # Make sure the model is in evaluation mode\n","sentence = [\"你好，世界\", \"你爱我我爱你，蜜雪冰城甜蜜蜜\", \"不要文革要改革 不要领袖要选票 不做奴才做公民\"] # Your Chinese sentence\n","translation = translate_sentence(model, tokenizer, sentence, device)\n","print(translation)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIUOy4ilaw4R","executionInfo":{"status":"ok","timestamp":1703398586245,"user_tz":480,"elapsed":567,"user":{"displayName":"Yifan Zhang","userId":"03805720886279282714"}},"outputId":"762a059b-65bd-4064-880c-27c91a4148cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[ 5349,     2,   907,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n","         65000, 65000, 65000, 65000, 65000, 65000],\n","        [  132, 28609, 41412,     2, 16351,  9128,  8677,  5257, 47772, 16351,\n","             0, 65000, 65000, 65000, 65000, 65000],\n","        [ 3494,  1926, 27436,   342,  1268,  3494, 10276,   342, 22696,   768,\n","          1090, 24359,  1880,  1090,  1860,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","['Hello, the World.', 'You love me. I love you, honey.', 'No reform of the Cultural Revolution, no vote for the leader, no slavery for the citizen.']\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"850620df17ac488f88a51d9e2d3f0707":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccc098f5f4974ac2809c774dc8179145","IPY_MODEL_35a2a7714d3a42caa7a082f32b4165bc","IPY_MODEL_7a39e5f32b484b52826de9928b635e79"],"layout":"IPY_MODEL_48943d199a20464c9fcb208832098c0e"}},"ccc098f5f4974ac2809c774dc8179145":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af606d616af14b1aaa0cb79c1c0e9013","placeholder":"​","style":"IPY_MODEL_d759ca2acc07453fa333a8ee9cfc872a","value":"  0%"}},"35a2a7714d3a42caa7a082f32b4165bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_86c9db158c3649649b1b1158e013053a","max":1580,"min":0,"orientation":"horizontal","style":"IPY_MODEL_918201f10e52400fa6de1462ebb149b9","value":0}},"7a39e5f32b484b52826de9928b635e79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c225b570b9439eabf4030009c8cd53","placeholder":"​","style":"IPY_MODEL_55733f66f1794a5ca9242ec4274cb361","value":" 0/1580 [00:03&lt;?, ?it/s]"}},"48943d199a20464c9fcb208832098c0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af606d616af14b1aaa0cb79c1c0e9013":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d759ca2acc07453fa333a8ee9cfc872a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86c9db158c3649649b1b1158e013053a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"918201f10e52400fa6de1462ebb149b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3c225b570b9439eabf4030009c8cd53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55733f66f1794a5ca9242ec4274cb361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f5f907fb54b4e8fb63f2650ec343b90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1eeb7478f1c84133aa757dde4615cdf1","IPY_MODEL_9833eca0a35e4716b0530c9a29ab5799","IPY_MODEL_1319d4b2b0ba450081cada2760de8176"],"layout":"IPY_MODEL_32747ae4668e4ce8945628ba697214a4"}},"1eeb7478f1c84133aa757dde4615cdf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07e0a9b4f88e425794e1791994a51bc4","placeholder":"​","style":"IPY_MODEL_3e6e53cec35f4c0583765276ea7fc1ac","value":"loss: 3.549027:  13%"}},"9833eca0a35e4716b0530c9a29ab5799":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_537d4b9f191f4075933c2126caa726cd","max":12640,"min":0,"orientation":"horizontal","style":"IPY_MODEL_afbf756d5541422d9c1d6aab05f9d204","value":1622}},"1319d4b2b0ba450081cada2760de8176":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3357ba838c4a48877c57bcab1bb370","placeholder":"​","style":"IPY_MODEL_6e514657e5c647f8ae091b1e0fbfb289","value":" 1622/12640 [09:47&lt;1:07:24,  2.72it/s]"}},"32747ae4668e4ce8945628ba697214a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07e0a9b4f88e425794e1791994a51bc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e6e53cec35f4c0583765276ea7fc1ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"537d4b9f191f4075933c2126caa726cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afbf756d5541422d9c1d6aab05f9d204":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee3357ba838c4a48877c57bcab1bb370":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e514657e5c647f8ae091b1e0fbfb289":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}